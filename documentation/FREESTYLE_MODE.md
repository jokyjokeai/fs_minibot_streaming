# Mode Freestyle AI - MiniBotPanel v3

## Vue d'ensemble

Le mode **Freestyle AI** permet au robot de g√©n√©rer des r√©ponses dynamiques et contextuelles aux questions hors-script des prospects, en utilisant Ollama (Mistral 7B) pour g√©n√©rer des r√©ponses naturelles et professionnelles.

---

## üéØ Fonctionnalit√©s

- ‚úÖ **G√©n√©ration dynamique** : R√©ponses cr√©√©es en temps r√©el selon le contexte
- ‚úÖ **Cache intelligent** : Questions fr√©quentes mises en cache (LRU, 100 entr√©es)
- ‚úÖ **Historique conversationnel** : Prend en compte les 5 derniers √©changes
- ‚úÖ **D√©tection automatique** : Identifie le type de question (objection, prix, info)
- ‚úÖ **Prompts adapt√©s** : 4 types de prompts selon la situation
- ‚úÖ **Validation stricte** : Limite 150 mots, suppression markdown, d√©tection mentions IA
- ‚úÖ **Int√©gration TTS** : G√©n√®re automatiquement l'audio de la r√©ponse

---

## üöÄ Utilisation dans les sc√©narios

### Format JSON

Pour activer le mode freestyle dans une √©tape, utilisez `"audio_type": "freestyle"` :

```json
{
  "freestyle_answer": {
    "audio_type": "freestyle",
    "voice": "julie",
    "barge_in": true,
    "timeout": 10,
    "max_turns": 3,
    "context": {
      "agent_name": "Julie",
      "company": "TechCorp",
      "product": "solution d'automatisation",
      "campaign_context": "Prospection B2B pour solution d'automatisation"
    },
    "intent_mapping": {
      "affirm": "question1",
      "question": "freestyle_answer",
      "deny": "objection",
      "*": "question1"
    }
  }
}
```

### Champs sp√©cifiques au mode freestyle

| Champ | Type | Requis | Description |
|-------|------|--------|-------------|
| `audio_type` | string | ‚úÖ | Doit √™tre `"freestyle"` |
| `voice` | string | ‚ùå | Voix clon√©e √† utiliser (d√©faut: "julie") |
| `max_turns` | int | ‚ùå | Nombre max d'√©changes freestyle (d√©faut: 3) |
| `context` | dict | ‚ùå | Contexte campagne pour g√©n√©ration |
| `timeout` | int | ‚ùå | Timeout √©coute en secondes (d√©faut: 10) |
| `barge_in` | bool | ‚ùå | Autoriser interruption (d√©faut: true) |
| `intent_mapping` | dict | ‚úÖ | Mapping intentions ‚Üí √©tapes suivantes |

**Note importante** : Le champ `message_text` n'est **PAS requis** pour les √©tapes freestyle (g√©n√©r√© dynamiquement par l'IA).

---

## üé® Exemple de sc√©nario complet

Voir `documentation/scenarios/exemple_freestyle.json` pour un exemple complet.

### Flow typique avec freestyle

```
Robot: "All√¥, bonjour Jean. Je suis Julie de TechCorp."
Client: "C'est pour quoi exactement ?"
  ‚Üí Intent "question" d√©tect√©
  ‚Üí Bascule vers √©tape "freestyle_answer"

Robot (IA Freestyle): "Je vous appelle pour vous pr√©senter notre solution
  d'automatisation qui aide les entreprises comme la v√¥tre √† gagner du temps
  sur les t√¢ches r√©p√©titives. Seriez-vous disponible pour en discuter ?"

Client: "Combien √ßa co√ªte ?"
  ‚Üí Intent "question" + keywords "combien/prix"
  ‚Üí Type de prompt "question_price" d√©tect√© automatiquement

Robot (IA Freestyle): "Le tarif d√©pend de vos besoins sp√©cifiques et du nombre
  d'utilisateurs. Puis-je vous proposer une d√©mo gratuite de 15 minutes pour
  vous montrer comment √ßa fonctionne et √©tablir un devis personnalis√© ?"

Client: "D'accord, pourquoi pas."
  ‚Üí Intent "affirm"
  ‚Üí Retour vers √©tape script√©e "question1"
```

---

## üß† Types de prompts automatiques

Le service FreestyleAI d√©tecte automatiquement le meilleur type de prompt selon les mots-cl√©s :

### 1. **default** (par d√©faut)
Question g√©n√©rale ou r√©ponse normale.

### 2. **objection**
D√©tect√© si pr√©sence de :
- "pas le temps", "pas int√©ress√©", "trop cher"
- "d√©j√†", "pas besoin", "√ßa va"
- "je r√©fl√©chis", "rappeler", "pas maintenant"

**Style de r√©ponse** : Empathique, reconna√Æt l'objection, propose solution.

### 3. **question_price**
D√©tect√© si pr√©sence de :
- "prix", "co√ªt", "combien", "tarif"
- "budget", "cher"

**Style de r√©ponse** : √âvite prix pr√©cis, explique personnalisation, propose RDV.

### 4. **question_info**
D√©tect√© si pr√©sence de :
- "comment", "pourquoi", "quoi"
- "c'est quoi", "qu'est-ce"

**Style de r√©ponse** : R√©ponse claire et pr√©cise, mentionne b√©n√©fice, propose RDV.

---

## üìä Statistiques disponibles

Le service FreestyleAI collecte des statistiques accessibles via :

```python
stats = robot.freestyle_service.get_stats()
```

**M√©triques retourn√©es** :
- `total_requests` : Nombre total de requ√™tes freestyle
- `successful_generations` : G√©n√©rations r√©ussies
- `cache_hits` / `cache_misses` : Performance du cache
- `cache_hit_rate_pct` : Taux de hit du cache en %
- `avg_generation_time_ms` : Temps moyen de g√©n√©ration (ms)
- `avg_response_length_words` : Longueur moyenne des r√©ponses (mots)
- `success_rate_pct` : Taux de succ√®s en %
- `cache_size` : Taille actuelle du cache
- `active_conversations` : Nombre de conversations actives
- `model` : Mod√®le Ollama utilis√©

---

## ‚öôÔ∏è Configuration

### Variables d'environnement

```bash
# Ollama (requis pour freestyle)
OLLAMA_MODEL=mistral:7b
OLLAMA_URL=http://localhost:11434
OLLAMA_TIMEOUT=10

# TTS (requis pour audio freestyle)
COQUI_MODEL=tts_models/multilingual/multi-dataset/xtts_v2
COQUI_USE_GPU=false
PRELOAD_MODELS=true
```

### Param√®tres par d√©faut (modifiables dans le code)

`system/services/freestyle_ai.py` :

```python
self.config = {
    "model": config.OLLAMA_MODEL,
    "max_response_words": 150,       # Max mots par r√©ponse
    "max_context_messages": 5,       # Historique conversationnel
    "temperature": 0.7,              # Cr√©ativit√© (0.0 = d√©terministe, 1.0 = cr√©atif)
    "cache_size": 100                # Nombre questions en cache
}
```

---

## üîß API du service FreestyleAI

### G√©n√©ration de r√©ponse

```python
response = freestyle_service.generate_response(
    call_uuid="abc-123",
    user_input="C'est pour quoi exactement ?",
    context={
        "agent_name": "Julie",
        "company": "TechCorp",
        "product": "solution CRM",
        "campaign_context": "Prospection B2B"
    },
    prompt_type="default"  # ou "objection", "question_price", "question_info"
)
```

### D√©tection automatique du type

```python
prompt_type = freestyle_service.detect_prompt_type("Combien √ßa co√ªte ?")
# ‚Üí "question_price"
```

### Nettoyage de l'historique

```python
# Nettoyer historique d'un appel sp√©cifique
freestyle_service.clear_conversation(call_uuid)

# Nettoyer tout le cache
freestyle_service.clear_all_cache()

# Nettoyer tous les historiques
freestyle_service.clear_all_conversations()
```

---

## üö® Limites et fallback

### R√®gles de validation strictes

1. **Maximum 150 mots** : Les r√©ponses trop longues sont tronqu√©es
2. **Suppression markdown** : `**`, `*`, `#`, `` ` `` automatiquement retir√©s
3. **D√©tection mentions IA** : Si "en tant qu'IA" d√©tect√© ‚Üí r√©ponse g√©n√©rique
4. **Timeout Ollama** : 10 secondes (configurable)

### Fallback automatique

Si g√©n√©ration √©choue, r√©ponse g√©n√©rique :

> "Je n'ai pas toutes les informations pour r√©pondre pr√©cis√©ment. Puis-je vous proposer un rendez-vous avec un expert qui pourra vous renseigner en d√©tail ?"

---

## üéØ Bonnes pratiques

### ‚úÖ DO

- **Mapper "question" ‚Üí freestyle** dans intent_mapping
- **Limiter max_turns** pour √©viter conversations trop longues
- **Fournir contexte riche** (agent_name, company, product)
- **Tester avec vraies questions** de prospects
- **Monitorer cache_hit_rate** pour optimiser prompts

### ‚ùå DON'T

- **Ne pas utiliser freestyle pour TOUTES les √©tapes** (co√ªteux en ressources)
- **Ne pas oublier intent_mapping** (sinon boucle infinie)
- **Ne pas laisser max_turns illimit√©** (pr√©voir sortie de secours)
- **Ne pas surcharger le contexte** (trop d'infos = r√©ponses vagues)

---

## üß™ Tests

### Test unitaire du service

```python
from system.services.freestyle_ai import FreestyleAI

freestyle = FreestyleAI()

# Test question simple
response = freestyle.generate_response(
    call_uuid="test-123",
    user_input="C'est quoi votre solution ?",
    context={"product": "CRM", "company": "TechCorp"}
)

print(response)
# ‚Üí "Notre solution CRM aide les entreprises √†..."
```

### Test de d√©tection

```python
# Objections
assert freestyle.detect_prompt_type("Pas le temps") == "objection"
assert freestyle.detect_prompt_type("Trop cher") == "objection"

# Prix
assert freestyle.detect_prompt_type("Combien √ßa co√ªte ?") == "question_price"
assert freestyle.detect_prompt_type("Quel est le tarif ?") == "question_price"

# Info
assert freestyle.detect_prompt_type("Comment √ßa marche ?") == "question_info"
```

---

## üìà Performances

### Benchmarks typiques (VPS 4 CPU, Ollama CPU mode)

- **Premi√®re g√©n√©ration** : ~2-3 secondes
- **Cache hit** : ~50-100 ms (instantan√©)
- **Taille moyenne r√©ponse** : 80-120 mots
- **Taux cache hit** : 15-30% (apr√®s warm-up)

### Optimisations possibles

1. **GPU** : Activer `COQUI_USE_GPU=true` (g√©n√©ration 5-10x plus rapide)
2. **Mod√®le plus petit** : `mistral:7b` ‚Üí `mistral:3b` (2x plus rapide)
3. **Cache Redis** : Impl√©menter cache partag√© entre instances
4. **Pr√©-g√©n√©ration** : Pr√©-g√©n√©rer r√©ponses pour top 50 questions

---

## üêõ D√©pannage

### Freestyle ne fonctionne pas

1. **V√©rifier Ollama actif** :
```bash
curl http://localhost:11434/api/tags
```

2. **V√©rifier logs** :
```bash
tail -f logs/minibot.log | grep Freestyle
```

3. **Tester service directement** :
```python
python3 -c "from system.services.freestyle_ai import FreestyleAI; f = FreestyleAI(); print(f.is_available)"
```

### R√©ponses incoh√©rentes

- **Augmenter temp√©rature** : `0.7` ‚Üí `0.9` (plus cr√©atif)
- **Diminuer temp√©rature** : `0.7` ‚Üí `0.3` (plus d√©terministe)
- **Am√©liorer prompts** dans `system/services/freestyle_ai.py`
- **Enrichir contexte** dans le sc√©nario JSON

### Cache ne fonctionne pas

- V√©rifier hash des questions (case-insensitive, trim)
- Cache est LRU (100 entr√©es max par d√©faut)
- Cache est en m√©moire (perdu au red√©marrage)

---

## üìö Ressources

- **Code source** : `system/services/freestyle_ai.py`
- **Exemple sc√©nario** : `documentation/scenarios/exemple_freestyle.json`
- **Architecture** : `documentation/BRIEF_PROJET.md` (lignes 163-209)
- **Ollama docs** : https://ollama.com/docs

---

## üîÆ √âvolutions futures

- [ ] Cache Redis distribu√©
- [ ] Mod√®les fine-tun√©s par industrie
- [ ] D√©tection √©motions (joie, frustration)
- [ ] Pr√©-g√©n√©ration top questions
- [ ] M√©triques Prometheus/Grafana
- [ ] A/B testing prompts
- [ ] Support GPT-4o (API OpenAI)
- [ ] Voice cloning temps r√©el (streaming TTS)

---

**Version** : MiniBotPanel v3
**Date** : Octobre 2024
**Auteur** : Claude Code + User
