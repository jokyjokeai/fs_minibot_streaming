# Rapport d'Analyse - Streaming Audio FreeSWITCH + Whisper
**Date:** 2025-11-16
**Objectif:** Impl√©menter streaming audio temps r√©el pour barge-in et transcription

---

## üìã R√âSUM√â EX√âCUTIF

### ‚úÖ CE QUI FONCTIONNE:
- **mod_audio_fork** compil√© et install√© sur FreeSWITCH ‚úÖ
- **WebSocket** serveur Python op√©rationnel ‚úÖ
- **Faster-Whisper GPU** s'initialise correctement ‚úÖ
- **Frames audio** re√ßus en continu (640 bytes @ 50fps) ‚úÖ

### ‚ùå PROBL√àMES BLOQUANTS:
1. **Direction audio INCORRECTE** - Capture silence au lieu de voix client
2. **Crash cuDNN** - Faster-Whisper crash pendant transcription finale
3. **Version novembre NON FONCTIONNELLE** - Code d√©sactiv√© car mod_audio_fork manquait

---

## üîç D√âCOUVERTES RECHERCHE WEB

### 1. mod_audio_fork vs mod_audio_stream

**mod_audio_fork** (Drachtio/Jambonz):
- Module **CUSTOM** n√©cessitant compilation FreeSWITCH avec libwebsockets
- Utilis√© en production par Jambonz
- Format: **L16 PCM** (Linear 16-bit)
- **Mix types support√©s:**
  - `read` - Inbound audio channel (caller)
  - `write` - Outbound audio channel (robot TTS)
  - `mixed` ou `stereo` - Both channels

**mod_audio_stream** (Alternative):
- Module **similaire** √† mod_audio_fork
- Commande: `uuid_audio_stream <uuid> start <wss-url> <mix-type> <sampling-rate>`
- Exemple: `uuid_audio_stream UUID start ws://... stereo 8k`

**Syntaxe drachtio/Jambonz:**
```javascript
await ep.forkAudioStart({
  wsUrl: 'ws://stt-service:8080/transcribe',
  mixType: 'mono',        // Caller audio only
  sampling: '16k',
  metadata: { callId, language }
});
```

### 2. Configuration Audio Direction

**Recherche web findings:**
- **Jambonz**: Utilise `mixType: "mono"` pour audio caller uniquement
- **WebSocket subprotocol**: `audio.drachtio.org`
- **Format audio**: 16-bit PCM, sample rates: 8k, 16k, 24k, 48k, 64k
- **Frames:** Binaires, pas de header, stream continu

### 3. Whisper Streaming Solutions

#### **UFAL whisper_streaming** (GitHub: ufal/whisper_streaming)
- Impl√©mentation streaming **officiellement reconnue**
- Backend recommand√©: **faster-whisper**
- **Local agreement policy** avec self-adaptive latency
- ‚úÖ Production-ready

#### **WhisperLive** (GitHub: collabora/WhisperLive)
- Backends: faster_whisper, tensorrt, openvino
- WebSocket server int√©gr√©
- Latence: ~100-300ms

#### **VoiceStreamAI** (GitHub: alesaccoia/VoiceStreamAI)
- WebSocket + VAD + Whisper
- Near real-time transcription
- Architecture similaire √† notre test

**CONCLUSION RECHERCHE:** Faster-Whisper est le backend **recommand√©** pour streaming, mais n√©cessite gestion VAD externe

---

## üì¶ ANALYSE CODE NOVEMBRE (streaming_asr.py)

### Architecture Trouv√©e

**Fichier:** `documentation/code_archive_novembre/streaming_asr.py`
**Date:** 8 novembre 2025

**Stack technique:**
```python
# Services utilis√©s
- WebRTC VAD (mode 2) - D√©tection parole/silence
- Vosk ASR - Transcription streaming (PAS Whisper!)
- WebSocket server (port 8080)
- Callbacks async pour barge-in
```

**Configuration VAD:**
```python
self.vad = webrtcvad.Vad(2)              # Mode 2 = balance qualit√©/r√©activit√©
self.sample_rate = 16000                  # 16kHz
self.frame_duration_ms = 30               # 30ms frames
self.silence_threshold = 0.8              # 800ms silence = fin parole
self.speech_start_threshold = 0.5         # 500ms parole = d√©but d√©tect√©
```

**Mod√®le:** Vosk (CPU-only, l√©ger, pas de GPU requis)

### √âtat de l'Impl√©mentation Novembre

**Fichier:** `documentation/code_archive_novembre/robot_freeswitch_nov6.py`

**Fonction `_enable_audio_fork()` - LIGNE 680-690:**
```python
def _enable_audio_fork(self, call_uuid: str):
    """Active le streaming audio vers le serveur WebSocket"""
    # TODO: uuid_audio_fork n'existe pas dans FreeSWITCH standard
    # Options pour streaming audio:
    # 1. mod_audio_fork (n√©cessite compilation custom)
    # 2. mod_avmd + mod_event_socket
    # 3. uuid_record + transcription post-call
    #
    # Pour l'instant: mode non-streaming (record + transcribe apr√®s)
    logger.debug(f"[{call_uuid[:8]}] Audio fork disabled (not supported yet)")
    return
```

**Code comment√© (ligne 698):**
```python
# cmd = f"uuid_audio_fork {call_uuid} start {websocket_url}"
#                                          ^^^^^
#                                          AUCUN param√®tre mono/read/stereo!
```

**‚ö†Ô∏è CONCLUSION CRITIQUE:**
- Leur version **NE MARCHAIT PAS** en novembre
- mod_audio_fork **n'√©tait PAS install√©** √† l'√©poque
- Fonction **d√©sactiv√©e** ‚Üí mode file-based utilis√©
- Pas de param√®tre direction audio dans leur code

---

## üß™ R√âSULTATS TEST 2025-11-16

### Test Effectu√©

**Fichier:** `test_whisper_streaming_call.py`
**Commande:** `uuid_audio_fork {uuid} start ws://127.0.0.1:8765/stream/{uuid} mono 16k`

### R√©sultats

#### ‚úÖ Succ√®s:
1. **mod_audio_fork activ√©:** `+OK Success`
2. **WebSocket connect√©:** Handshake r√©ussi, connection OPEN
3. **Faster-Whisper GPU init:** Mod√®le charg√© sans erreur
4. **Frames re√ßus:** ~450 frames en 10 secondes (correct pour 50fps)

#### ‚ùå √âchecs:
1. **Direction audio:**
   - Tous frames: `ff ff ff ff` (silence/bruit)
   - RMS: 0.95 (quasi-silence)
   - **ALORS QUE** client a parl√© ("c'est moi Richard")
   - **DIAGNOSTIC:** Mode `mono` capture mauvais leg ou silence

2. **Crash cuDNN:**
   ```
   Unable to load libcudnn_ops.so.9.1.0
   Invalid handle. Cannot load symbol cudnnCreateTensorDescriptor
   [Exit code: 134 - SIGABRT]
   ```
   - Crash pendant transcription finale
   - Whisper init OK, mais crash sur transcription avec VAD

3. **VAD Whisper filtre tout:**
   - VAD d√©tecte 100% silence
   - 0 transcription g√©n√©r√©e
   - Buffer vid√© syst√©matiquement

---

## üîß SOLUTIONS POSSIBLES

### Option A: Tester Diff√©rents Mix Types mod_audio_fork

**Hypoth√®se:** `mono` capture mauvais leg

**Tests √† faire:**
```bash
# Test 1: read (inbound uniquement)
uuid_audio_fork {uuid} start ws://... read 16k

# Test 2: mixed (les 2 legs)
uuid_audio_fork {uuid} start ws://... mixed 16k

# Test 3: stereo (2 channels s√©par√©s)
uuid_audio_fork {uuid} start ws://... stereo 16k
```

**Attentes:**
- `read` devrait capturer **UNIQUEMENT** voix client
- `mixed` capturera robot + client (√† filtrer c√¥t√© serveur)
- `stereo` donnera 2 channels s√©par√©s

### Option B: Utiliser Vosk au lieu de Whisper

**Avantages:**
- ‚úÖ Comme version novembre (√©prouv√©)
- ‚úÖ Pas de GPU ‚Üí pas de cuDNN crash
- ‚úÖ Latence plus faible (~50-100ms vs 200-500ms)
- ‚úÖ Pas d'hallucinations
- ‚úÖ Mod√®le l√©ger (CPU suffisant)

**Inconv√©nients:**
- ‚ùå Pr√©cision inf√©rieure √† Whisper
- ‚ùå Moins de langues support√©es
- ‚ùå Pas de ponctuation automatique

**Impl√©mentation:**
```python
from vosk import Model, KaldiRecognizer

model = Model("models/vosk-model-fr-0.22")
recognizer = KaldiRecognizer(model, 16000)

# Streaming
for audio_chunk in stream:
    if recognizer.AcceptWaveform(audio_chunk):
        result = json.loads(recognizer.Result())
        text = result["text"]  # Transcription finale
    else:
        partial = json.loads(recognizer.PartialResult())
        text = partial["partial"]  # Transcription partielle
```

### Option C: Fixer cuDNN pour Whisper

**Probl√®me identifi√©:**
- cuDNN 9.x incompatible avec certaines op√©rations Faster-Whisper
- Crash sp√©cifique sur `cudnnCreateTensorDescriptor`

**Solutions:**
1. **Downgrade cuDNN 9.x ‚Üí 8.x**
   ```bash
   pip uninstall nvidia-cudnn-cu12
   pip install nvidia-cudnn-cu12==8.9.7.29
   ```

2. **D√©sactiver VAD Whisper** (cause probable du crash)
   ```python
   segments, info = self.model.transcribe(
       audio,
       language="fr",
       vad_filter=False,  # ‚Üê D√©sactiver VAD
       beam_size=1
   )
   ```

3. **Utiliser CPU pour transcription** (lent mais stable)
   ```python
   model = WhisperModel("small", device="cpu", compute_type="int8")
   ```

### Option D: Hybrid - Vosk streaming + Whisper final

**Concept:**
- **Vosk** pour d√©tections temps r√©el (barge-in, VAD)
- **Whisper** pour transcription finale pr√©cise (post-call)

**Avantages:**
- ‚úÖ R√©activit√© Vosk (<100ms)
- ‚úÖ Pr√©cision Whisper (offline)
- ‚úÖ Pas de cuDNN crash en live
- ‚úÖ Best of both worlds

---

## üìä COMPARAISON SOLUTIONS

| Crit√®re | A: Fix mod_audio_fork | B: Vosk streaming | C: Fix cuDNN Whisper | D: Hybrid |
|---------|----------------------|-------------------|---------------------|-----------|
| **Complexit√©** | Faible (test params) | Moyenne | √âlev√©e | √âlev√©e |
| **Temps impl.** | 1-2h | 4-6h | 2-4h | 6-10h |
| **Latence** | 200-500ms | 50-100ms | 200-500ms | 50-100ms (live) |
| **Pr√©cision** | √âlev√©e (Whisper) | Moyenne (Vosk) | √âlev√©e (Whisper) | √âlev√©e (Whisper offline) |
| **Stabilit√©** | ? (√† tester) | ‚úÖ √âprouv√© | ‚ùå Crash cuDNN | ‚úÖ Stable |
| **GPU requis** | ‚úÖ Oui | ‚ùå Non | ‚úÖ Oui | ‚úÖ Oui (offline seulement) |
| **Production ready** | üü° Inconnu | ‚úÖ Oui | ‚ùå Non (instable) | ‚úÖ Oui |

---

## üéØ RECOMMANDATION FINALE

### Plan d'Action Recommand√©

#### **PHASE 1: Tests Direction Audio (1-2h)** ‚≠ê PRIORIT√â

**Objectif:** Identifier le bon mix type

**Actions:**
1. Modifier `test_whisper_streaming_call.py`
2. Tester s√©quentiellement: `read`, `mixed`, `stereo`
3. Pour chaque test:
   - Appeler, parler "c'est moi Richard"
   - V√©rifier RMS audio (>500 = parole d√©tect√©e)
   - V√©rifier transcriptions

**Crit√®re succ√®s:** RMS >500 ET transcriptions correctes

#### **PHASE 2: Migration Vosk (4-6h)** üîÑ

**SI Phase 1 √©choue OU instabilit√© Whisper:**

**Actions:**
1. Installer Vosk: `pip install vosk`
2. T√©l√©charger mod√®le fran√ßais: `vosk-model-fr-0.22`
3. Adapter `test_whisper_streaming_call.py` pour Vosk
4. Tester streaming complet

**Avantages:**
- Architecture √©prouv√©e (novembre)
- Pas de cuDNN crash
- Latence optimale

#### **PHASE 3: Int√©gration Production (2-4h)** ‚úÖ

**Apr√®s succ√®s Phase 1 OU 2:**

**Actions:**
1. Cr√©er `system/services/live_streaming_stt.py`
2. Int√©grer dans `robot_freeswitch.py`:
   - D√©marrer serveur WebSocket au boot
   - Activer mod_audio_fork par call
   - Callbacks barge-in
3. Tests charge (multiple calls)
4. Documentation

---

## üí° D√âCISION ARCHITECTURE

### Si mod_audio_fork `read` mode FONCTIONNE:

**‚Üí GARDER Whisper** (pr√©cision maximale)
- Fixer cuDNN (downgrade 8.x ou d√©sactiver VAD)
- Production avec Faster-Whisper GPU
- Latence acceptable (~200-300ms)

### Si mod_audio_fork direction PROBL√âMATIQUE:

**‚Üí MIGRER vers Vosk** (stabilit√© maximale)
- Comme novembre (architecture √©prouv√©e)
- CPU-only (pas de cuDNN)
- Latence optimale (~50-100ms)
- Sacrifice pr√©cision pour r√©activit√©

### Si TOUT √©choue:

**‚Üí REVENIR file-based optimis√©**
- Version production actuelle marche
- Optimiser barge-in (VAD + snapshots 100ms)
- Abandonner streaming pour v4

---

## üìù PROCHAINES √âTAPES IMM√âDIATES

1. **Tester `read` mode** - 15 min
2. **Si OK:** Fixer cuDNN - 1h
3. **Si KO:** Tester `mixed` et `stereo` - 30 min
4. **Si tous KO:** Basculer Vosk - 4h

**D√©cision:** √Ä prendre apr√®s tests Phase 1

---

## üìö R√âF√âRENCES

### Documentation
- mod_audio_fork README: https://github.com/drachtio/drachtio-freeswitch-modules
- Jambonz listen verb: https://www.jambonz.org/docs/webhooks/listen/
- UFAL whisper_streaming: https://github.com/ufal/whisper_streaming
- Faster-Whisper: https://github.com/SYSTRAN/faster-whisper
- Vosk: https://alphacephei.com/vosk/

### Fichiers Code
- `documentation/code_archive_novembre/streaming_asr.py` - Architecture Vosk
- `documentation/code_archive_novembre/robot_freeswitch_nov6.py` - Int√©gration (d√©sactiv√©e)
- `test_whisper_streaming_call.py` - Test actuel

---

**Rapport cr√©√©:** 2025-11-16 12:45
**Auteur:** Analyse approfondie streaming audio
**Statut:** En attente d√©cision tests Phase 1
