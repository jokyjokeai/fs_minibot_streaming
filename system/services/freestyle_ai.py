"""
Freestyle AI Service - MiniBotPanel v3

Service de gÃ©nÃ©ration de rÃ©ponses dynamiques (freestyle) pour conversations hors-script.

FonctionnalitÃ©s:
- GÃ©nÃ©ration rÃ©ponses contextuelles via Ollama
- Cache questions frÃ©quentes (mÃ©moire + optionnel Redis)
- Historique conversationnel (5 derniers Ã©changes)
- Limites intelligentes (max words, max turns)
- Prompts engineering pour commercial naturel
- DÃ©tection objectifs conversationnels

Utilisation:
    from system.services.freestyle_ai import FreestyleAI

    freestyle = FreestyleAI(ollama_service, tts_service)

    response_text = freestyle.generate_response(
        call_uuid="abc-123",
        user_input="C'est pour quoi exactement ?",
        context={
            "campaign": "Prospection B2B",
            "product": "Solution CRM",
            "agent_name": "Julie"
        }
    )
"""

import json
import time
import hashlib
from typing import Dict, Any, Optional, List
from collections import OrderedDict
from datetime import datetime

from system.config import config
from system.logger import get_logger

logger = get_logger(__name__)

# Import Ollama avec fallback
try:
    import ollama
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False
    logger.warning("âš ï¸ Ollama not available for freestyle")


class FreestyleAI:
    """
    Service de gÃ©nÃ©ration de rÃ©ponses freestyle (hors-script).
    Utilise Ollama pour gÃ©nÃ©rer des rÃ©ponses naturelles et contextuelles.
    """

    def __init__(self, ollama_service=None, tts_service=None):
        """
        Initialise le service Freestyle AI.

        Args:
            ollama_service: Service Ollama NLP (optionnel, crÃ©Ã© auto si None)
            tts_service: Service Coqui TTS (optionnel)
        """
        logger.info("Initializing FreestyleAI...")

        self.ollama = ollama_service
        self.tts = tts_service
        self.is_available = OLLAMA_AVAILABLE

        # Configuration
        self.config = {
            "model": config.OLLAMA_MODEL,
            "url": config.OLLAMA_URL,
            "timeout": config.OLLAMA_TIMEOUT,
            "max_response_words": 150,  # Max mots par rÃ©ponse
            "max_context_messages": 5,  # Historique conversationnel
            "temperature": 0.7,          # CrÃ©ativitÃ© modÃ©rÃ©e
            "cache_size": 100            # Nombre questions en cache
        }

        # Cache questions frÃ©quentes (LRU)
        self._cache = OrderedDict()
        self._cache_hits = 0
        self._cache_misses = 0

        # Historique conversationnel par call_uuid
        self._conversations = {}  # {call_uuid: [messages]}

        # Statistiques
        self.stats = {
            "total_requests": 0,
            "successful_generations": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "avg_generation_time_ms": 0.0,
            "avg_response_length_words": 0.0
        }

        # Prompts systÃ¨me par type d'interaction
        self._system_prompts = {
            "default": """Tu es un assistant commercial au tÃ©lÃ©phone nommÃ© {agent_name}.
Tu travailles pour {company} et tu proposes {product}.

RÃˆGLES STRICTES:
1. RÃ©ponds en FRANÃ‡AIS naturel et professionnel
2. Maximum 2-3 phrases (â‰¤150 mots)
3. RÃ©ponds DIRECTEMENT Ã  la question posÃ©e
4. Ton amical mais pas trop familier
5. RamÃ¨ne subtilement vers l'offre/rendez-vous
6. Ne mentionne JAMAIS que tu es une IA
7. Si tu ne sais pas, dis "Je vais vÃ©rifier et revenir vers vous"

INTERDICTIONS:
- Pas de markdown, pas de formatage
- Pas de "En tant qu'IA..."
- Pas de rÃ©ponses vagues
- Pas de questions multiples

Contexte campagne: {campaign_context}""",

            "objection": """Tu es un commercial expÃ©rimentÃ© nommÃ© {agent_name}.
Le prospect vient de faire une objection.

Ta rÃ©ponse doit:
1. ReconnaÃ®tre l'objection ("Je comprends...")
2. Apporter un Ã©lÃ©ment rassurant
3. Proposer une solution concrÃ¨te
4. Garder la porte ouverte

Maximum 2 phrases, ton empathique.

Contexte: {campaign_context}""",

            "question_price": """Tu es commercial pour {company}.
Le prospect demande des informations sur le prix.

Ta rÃ©ponse doit:
1. Ne PAS donner de prix prÃ©cis par tÃ©lÃ©phone
2. Expliquer que Ã§a dÃ©pend des besoins
3. Proposer une dÃ©mo/RDV pour personnaliser

Maximum 2 phrases, professionnel mais pas Ã©vasif.

Contexte: {campaign_context}""",

            "question_info": """Tu es {agent_name}, expert de {product}.
Le prospect demande des infos techniques/pratiques.

Ta rÃ©ponse doit:
1. Donner une rÃ©ponse claire et prÃ©cise
2. Rester simple (pas de jargon)
3. Mentionner un bÃ©nÃ©fice concret
4. Proposer d'en parler en dÃ©tail lors d'un RDV

Maximum 3 phrases.

Contexte: {campaign_context}"""
        }

        # Questions fermÃ©es pour retour au rail (Phase 6+)
        # VariÃ©es pour Ã©viter rÃ©pÃ©tition robotique
        self._rail_return_questions = {
            "general": [
                "Ã‡a vous parle ?",
                "C'est clair pour vous ?",
                "Vous me suivez ?",
                "Ã‡a rÃ©pond Ã  votre question ?",
                "C'est bon pour vous ?",
                "Vous voyez l'idÃ©e ?",
                "Ã‡a fait sens ?",
                "D'accord ?"
            ],
            "after_objection": [
                "Vous Ãªtes d'accord avec moi ?",
                "Ã‡a vous rassure un peu ?",
                "Vous comprenez mieux maintenant ?",
                "Ã‡a vous semble plus clair ?",
                "C'est mieux comme Ã§a ?",
                "Ã‡a change votre vision ?",
                "Ã‡a vous parle davantage ?"
            ],
            "after_info": [
                "C'est plus clair pour vous ?",
                "Vous avez toutes les infos qu'il vous faut ?",
                "Ã‡a rÃ©pond Ã  votre question ?",
                "Vous voulez que je prÃ©cise un point ?",
                "C'est bon de votre cÃ´tÃ© ?",
                "Vous y voyez plus clair ?"
            ],
            "after_price": [
                "Vous voyez que c'est personnalisable ?",
                "Ã‡a mÃ©rite qu'on en parle, non ?",
                "Vous Ãªtes partant pour en discuter ?",
                "On peut creuser ensemble ?",
                "Ã‡a vous intÃ©resse d'en savoir plus ?"
            ]
        }

        # Index pour rotation des questions (Ã©viter rÃ©pÃ©tition)
        self._rail_question_indices = {
            category: 0 for category in self._rail_return_questions.keys()
        }

        if not self.is_available:
            logger.warning("ðŸš« FreestyleAI not available - Ollama missing")
            return

        logger.info("âœ… FreestyleAI initialized")

    def generate_response(
        self,
        call_uuid: str,
        user_input: str,
        context: Optional[Dict[str, Any]] = None,
        prompt_type: str = "default"
    ) -> str:
        """
        GÃ©nÃ¨re une rÃ©ponse freestyle contextuelle.

        Args:
            call_uuid: UUID de l'appel
            user_input: Question/remarque du prospect
            context: Contexte campagne (agent_name, company, product, etc.)
            prompt_type: Type de prompt (default, objection, question_price, etc.)

        Returns:
            Texte de la rÃ©ponse gÃ©nÃ©rÃ©e
        """
        self.stats["total_requests"] += 1

        if not self.is_available or not OLLAMA_AVAILABLE:
            logger.error("FreestyleAI not available")
            return "Je vous remercie pour votre question. Un conseiller va vous rappeler."

        # Nettoyer input
        user_input = user_input.strip()
        if not user_input:
            return "Je vous Ã©coute."

        # Context par dÃ©faut
        if context is None:
            context = {}

        context.setdefault("agent_name", "Julie")
        context.setdefault("company", "notre entreprise")
        context.setdefault("product", "nos solutions")
        context.setdefault("campaign_context", "Appel de prospection commercial")

        # 1. VÃ©rifier cache
        cache_key = self._get_cache_key(user_input, context)
        cached_response = self._get_from_cache(cache_key)

        if cached_response:
            self.stats["cache_hits"] += 1
            logger.debug(f"Cache HIT for: {user_input[:50]}...")
            return cached_response

        self.stats["cache_misses"] += 1

        # 2. Construire historique conversation
        conversation_history = self._get_conversation_history(call_uuid)

        # 3. GÃ©nÃ©rer rÃ©ponse avec Ollama
        start_time = time.time()

        try:
            response = self._generate_with_ollama(
                user_input=user_input,
                context=context,
                conversation_history=conversation_history,
                prompt_type=prompt_type
            )

            generation_time = (time.time() - start_time) * 1000

            # Valider rÃ©ponse
            response = self._validate_response(response)

            # Sauvegarder dans historique
            self._add_to_conversation(call_uuid, user_input, response)

            # Sauvegarder dans cache
            self._add_to_cache(cache_key, response)

            # Stats
            self.stats["successful_generations"] += 1
            self._update_avg_generation_time(generation_time)
            self._update_avg_response_length(response)

            logger.info(
                f"Freestyle generated ({generation_time:.0f}ms): "
                f"{user_input[:30]}... â†’ {response[:50]}..."
            )

            return response

        except Exception as e:
            logger.error(f"Freestyle generation failed: {e}", exc_info=True)
            return "Merci pour votre question. Puis-je vous proposer un rendez-vous pour en discuter ?"

    def _generate_with_ollama(
        self,
        user_input: str,
        context: Dict[str, Any],
        conversation_history: List[Dict[str, str]],
        prompt_type: str
    ) -> str:
        """GÃ©nÃ¨re rÃ©ponse avec Ollama"""

        # SÃ©lectionner prompt systÃ¨me
        system_prompt_template = self._system_prompts.get(
            prompt_type,
            self._system_prompts["default"]
        )

        # Formater prompt avec contexte
        system_prompt = system_prompt_template.format(**context)

        # Construire messages
        messages = [{"role": "system", "content": system_prompt}]

        # Ajouter historique
        for msg in conversation_history[-self.config["max_context_messages"]:]:
            messages.append({"role": "user", "content": msg["user"]})
            messages.append({"role": "assistant", "content": msg["assistant"]})

        # Ajouter question actuelle
        messages.append({"role": "user", "content": user_input})

        # Appel Ollama
        response = ollama.chat(
            model=self.config["model"],
            messages=messages,
            options={
                "temperature": self.config["temperature"],
                "num_predict": 200,  # Max tokens
                "top_p": 0.9,
                "top_k": 40
            }
        )

        # Extraire texte
        response_text = response.get("message", {}).get("content", "").strip()

        return response_text

    def _validate_response(self, response: str) -> str:
        """Valide et nettoie la rÃ©ponse gÃ©nÃ©rÃ©e"""

        # Supprimer markdown
        response = response.replace("**", "").replace("*", "")
        response = response.replace("#", "").replace("`", "")

        # Supprimer mentions IA
        ai_mentions = [
            "en tant qu'ia",
            "en tant qu'intelligence artificielle",
            "je suis une ia",
            "je suis un modÃ¨le",
            "en tant que modÃ¨le"
        ]
        response_lower = response.lower()
        for mention in ai_mentions:
            if mention in response_lower:
                logger.warning(f"IA mention detected in response: {mention}")
                response = "Merci pour votre question. Laissez-moi vous expliquer notre offre en dÃ©tail."

        # Limiter longueur
        words = response.split()
        if len(words) > self.config["max_response_words"]:
            logger.warning(f"Response too long ({len(words)} words), truncating")
            response = " ".join(words[:self.config["max_response_words"]]) + "."

        # Nettoyer espaces multiples
        response = " ".join(response.split())

        return response

    def _get_cache_key(self, user_input: str, context: Dict[str, Any]) -> str:
        """GÃ©nÃ¨re clÃ© de cache"""
        # Hash basÃ© sur input + contexte principal
        cache_string = f"{user_input.lower()}_{context.get('product', '')}_{context.get('company', '')}"
        return hashlib.md5(cache_string.encode()).hexdigest()

    def _get_from_cache(self, key: str) -> Optional[str]:
        """RÃ©cupÃ¨re rÃ©ponse du cache"""
        if key in self._cache:
            # Move to end (LRU)
            self._cache.move_to_end(key)
            self._cache_hits += 1
            return self._cache[key]
        return None

    def _add_to_cache(self, key: str, response: str):
        """Ajoute rÃ©ponse au cache"""
        self._cache[key] = response
        self._cache.move_to_end(key)

        # Limite cache size (LRU eviction)
        if len(self._cache) > self.config["cache_size"]:
            self._cache.popitem(last=False)

    def _get_conversation_history(self, call_uuid: str) -> List[Dict[str, str]]:
        """RÃ©cupÃ¨re historique conversation"""
        return self._conversations.get(call_uuid, [])

    def _add_to_conversation(self, call_uuid: str, user_input: str, response: str):
        """Ajoute Ã©change Ã  l'historique"""
        if call_uuid not in self._conversations:
            self._conversations[call_uuid] = []

        self._conversations[call_uuid].append({
            "user": user_input,
            "assistant": response,
            "timestamp": datetime.now().isoformat()
        })

        # Limite historique
        max_history = self.config["max_context_messages"] * 2
        if len(self._conversations[call_uuid]) > max_history:
            self._conversations[call_uuid] = self._conversations[call_uuid][-max_history:]

    def clear_conversation(self, call_uuid: str):
        """Nettoie historique d'un appel"""
        if call_uuid in self._conversations:
            del self._conversations[call_uuid]
            logger.debug(f"Conversation history cleared for {call_uuid}")

    def _update_avg_generation_time(self, time_ms: float):
        """Met Ã  jour temps de gÃ©nÃ©ration moyen"""
        total = self.stats["successful_generations"]
        current_avg = self.stats["avg_generation_time_ms"]

        self.stats["avg_generation_time_ms"] = (
            (current_avg * (total - 1) + time_ms) / total
        )

    def _update_avg_response_length(self, response: str):
        """Met Ã  jour longueur rÃ©ponse moyenne"""
        word_count = len(response.split())
        total = self.stats["successful_generations"]
        current_avg = self.stats["avg_response_length_words"]

        self.stats["avg_response_length_words"] = (
            (current_avg * (total - 1) + word_count) / total
        )

    def detect_prompt_type(self, user_input: str) -> str:
        """
        DÃ©tecte automatiquement le type de prompt selon l'input.

        Args:
            user_input: Texte du prospect

        Returns:
            Type de prompt (default, objection, question_price, question_info)
        """
        user_input_lower = user_input.lower()

        # DÃ©tection objection
        objection_keywords = [
            "pas le temps", "pas intÃ©ressÃ©", "trop cher", "dÃ©jÃ ",
            "pas besoin", "Ã§a va", "je rÃ©flÃ©chis", "rappeler", "pas maintenant"
        ]
        if any(kw in user_input_lower for kw in objection_keywords):
            return "objection"

        # DÃ©tection question prix
        price_keywords = ["prix", "coÃ»t", "combien", "tarif", "budget", "cher"]
        if any(kw in user_input_lower for kw in price_keywords):
            return "question_price"

        # DÃ©tection question info
        question_keywords = ["comment", "pourquoi", "quoi", "c'est quoi", "qu'est-ce"]
        if any(kw in user_input_lower for kw in question_keywords):
            return "question_info"

        return "default"

    def generate_rail_return_question(
        self,
        prompt_type: str = "general",
        include_in_response: bool = True
    ) -> str:
        """
        GÃ©nÃ¨re une question fermÃ©e variÃ©e pour retour au rail (Phase 6+).

        AprÃ¨s avoir rÃ©pondu Ã  une objection/question en mode freestyle,
        cette mÃ©thode gÃ©nÃ¨re une question fermÃ©e (oui/non) pour reprendre
        le contrÃ´le de la conversation et ramener vers le rail.

        Les questions sont variÃ©es et tournent pour Ã©viter la rÃ©pÃ©tition robotique.

        Args:
            prompt_type: Type de contexte (objection, question_price, question_info, default)
            include_in_response: Si True, retourne question complÃ¨te. Si False, juste la question

        Returns:
            Question fermÃ©e variÃ©e

        Example:
            >>> freestyle.generate_rail_return_question("objection")
            "Ã‡a vous rassure un peu ?"

            >>> freestyle.generate_rail_return_question("question_price")
            "Ã‡a mÃ©rite qu'on en parle, non ?"
        """
        # Mapper prompt_type vers catÃ©gorie de questions
        category_map = {
            "objection": "after_objection",
            "question_price": "after_price",
            "question_info": "after_info",
            "default": "general"
        }

        category = category_map.get(prompt_type, "general")

        # RÃ©cupÃ©rer liste des questions pour cette catÃ©gorie
        questions = self._rail_return_questions.get(category, self._rail_return_questions["general"])

        # Rotation des questions (Ã©viter rÃ©pÃ©tition)
        index = self._rail_question_indices[category]
        question = questions[index]

        # IncrÃ©menter index (avec wrap-around)
        self._rail_question_indices[category] = (index + 1) % len(questions)

        logger.debug(f"Rail return question ({category}): {question}")

        return question

    def generate_response_with_rail_return(
        self,
        call_uuid: str,
        user_input: str,
        context: Optional[Dict[str, Any]] = None,
        prompt_type: str = "default"
    ) -> str:
        """
        GÃ©nÃ¨re une rÃ©ponse freestyle AVEC question de retour au rail (Phase 6+).

        Cette mÃ©thode combine:
        1. RÃ©ponse freestyle Ã  l'objection/question
        2. Question fermÃ©e variÃ©e pour retour au rail

        Args:
            call_uuid: UUID de l'appel
            user_input: Question/remarque du prospect
            context: Contexte campagne
            prompt_type: Type de prompt

        Returns:
            RÃ©ponse complÃ¨te avec question de retour

        Example:
            Input: "C'est trop cher pour moi"
            Output: "Je comprends votre prÃ©occupation. En rÃ©alitÃ©, nos clients
                     Ã©conomisent en moyenne 30% sur leurs coÃ»ts actuels.
                     Ã‡a vous rassure un peu ?"
        """
        # 1. GÃ©nÃ©rer rÃ©ponse principale
        main_response = self.generate_response(
            call_uuid=call_uuid,
            user_input=user_input,
            context=context,
            prompt_type=prompt_type
        )

        # 2. Ajouter question de retour au rail
        rail_question = self.generate_rail_return_question(prompt_type)

        # 3. Combiner (avec espace si besoin)
        if main_response.endswith((".", "!", "?")):
            combined = f"{main_response} {rail_question}"
        else:
            combined = f"{main_response}. {rail_question}"

        logger.info(f"Freestyle with rail return: {combined[:80]}...")

        return combined

    def get_stats(self) -> Dict[str, Any]:
        """Retourne statistiques freestyle"""
        cache_hit_rate = (
            (self.stats["cache_hits"] / self.stats["total_requests"] * 100)
            if self.stats["total_requests"] > 0 else 0
        )

        success_rate = (
            (self.stats["successful_generations"] / self.stats["total_requests"] * 100)
            if self.stats["total_requests"] > 0 else 0
        )

        return {
            **self.stats,
            "is_available": self.is_available,
            "cache_hit_rate_pct": round(cache_hit_rate, 1),
            "success_rate_pct": round(success_rate, 1),
            "cache_size": len(self._cache),
            "active_conversations": len(self._conversations),
            "model": self.config["model"]
        }

    def clear_all_cache(self):
        """Nettoie tout le cache"""
        self._cache.clear()
        logger.info("Freestyle cache cleared")

    def clear_all_conversations(self):
        """Nettoie tous les historiques"""
        self._conversations.clear()
        logger.info("All conversation histories cleared")
