"""
Streaming ASR Service - MiniBotPanel v3

Service de transcription audio temps r√©el avec d√©tection d'activit√© vocale (VAD).
Adapt√© de live_asr_vad.py pour FreeSWITCH.

Architecture:
- Serveur WebSocket qui re√ßoit audio depuis FreeSWITCH
- WebRTC VAD pour d√©tection parole/silence
- Vosk ASR pour transcription streaming
- Callbacks pour barge-in et IA Freestyle

Utilisation:
    from system.services.streaming_asr import StreamingASR

    asr = StreamingASR()

    # D√©marrer serveur
    await asr.start_server()

    # Register callback pour un call
    asr.register_callback(call_uuid, callback_function)
"""

import asyncio
import json
import time
import struct
import numpy as np
from typing import Dict, Optional, Any, Callable
from pathlib import Path

try:
    import websockets
    WEBSOCKETS_AVAILABLE = True
except ImportError:
    WEBSOCKETS_AVAILABLE = False

try:
    import webrtcvad
    VAD_AVAILABLE = True
except ImportError:
    VAD_AVAILABLE = False

try:
    from vosk import Model, KaldiRecognizer
    VOSK_AVAILABLE = True
except ImportError:
    VOSK_AVAILABLE = False

try:
    from scipy import signal
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

# NoiseReduce import d√©sactiv√© (non utilis√© - enable_noisereduce = False)
# try:
#     import noisereduce as nr
#     NOISEREDUCE_AVAILABLE = True
# except ImportError:
#     NOISEREDUCE_AVAILABLE = False
NOISEREDUCE_AVAILABLE = False  # Forc√© √† False

from system.config import config
from system.logger import get_logger

logger = get_logger(__name__)


class StreamingASR:
    """
    Service de transcription streaming avec VAD pour FreeSWITCH.
    G√®re le barge-in et la d√©tection de silence.
    """

    def __init__(self):
        """Initialise le service streaming ASR"""
        logger.info("Initializing StreamingASR...")

        self.is_available = WEBSOCKETS_AVAILABLE and VAD_AVAILABLE and VOSK_AVAILABLE

        if not self.is_available:
            missing = []
            if not WEBSOCKETS_AVAILABLE:
                missing.append("websockets")
            if not VAD_AVAILABLE:
                missing.append("webrtcvad")
            if not VOSK_AVAILABLE:
                missing.append("vosk")
            logger.warning(f"üö´ StreamingASR not available - missing: {', '.join(missing)}")
            return

        # Configuration VAD
        self.vad = webrtcvad.Vad(2)  # Mode 2 = balance qualit√©/r√©activit√©
        self.sample_rate = config.VOSK_SAMPLE_RATE  # 16000 Hz
        self.frame_duration_ms = 30  # 30ms frames
        self.frame_size = int(self.sample_rate * self.frame_duration_ms / 1000)

        # Seuils (lus depuis config pour coh√©rence)
        self.silence_threshold = config.VAD_SILENCE_THRESHOLD_MS / 1000.0  # 500ms ‚Üí 0.5s (optimis√© bruits)
        self.speech_start_threshold = config.VAD_SPEECH_START_THRESHOLD_MS / 1000.0  # 500ms ‚Üí 0.5s

        # Audio filters D√âSACTIV√âS (causaient des probl√®mes de transcription)
        # Les filtres high-pass et noise gate ont √©t√© supprim√©s
        logger.info("‚ÑπÔ∏è Audio filters disabled (raw audio to Vosk)")

        # Mod√®le Vosk
        self.model = None
        self.recognizers = {}  # {call_uuid: KaldiRecognizer}

        # √âtat streams
        self.active_streams = {}  # {call_uuid: stream_info}
        self.callbacks = {}  # {call_uuid: callback_function}

        # Serveur WebSocket
        self.websocket_server = None
        self.server_task = None

        # Statistiques
        self.stats = {
            "active_streams": 0,
            "total_frames_processed": 0,
            "speech_frames": 0,
            "silence_frames": 0,
            "transcriptions": 0,
            "avg_latency_ms": 0.0
        }

        # Charger mod√®le Vosk
        self._load_vosk_model()

        logger.info(f"{'‚úÖ' if self.is_available else '‚ùå'} StreamingASR initialized")

    def _load_vosk_model(self):
        """Charge le mod√®le Vosk"""
        try:
            model_path = Path(config.VOSK_MODEL_PATH)

            if not model_path.exists():
                logger.error(f"Vosk model not found: {model_path}")
                self.is_available = False
                return

            logger.info(f"üß† Loading Vosk model from {model_path}")
            start_time = time.time()

            self.model = Model(str(model_path))

            load_time = time.time() - start_time
            logger.info(f"‚úÖ Vosk model loaded in {load_time:.2f}s")

        except Exception as e:
            logger.error(f"‚ùå Failed to load Vosk model: {e}")
            self.is_available = False

    async def start_server(self, host: str = "127.0.0.1", port: int = 8080):
        """
        D√©marre le serveur WebSocket pour recevoir audio depuis FreeSWITCH

        Args:
            host: Host √† √©couter
            port: Port √† √©couter
        """
        if not self.is_available:
            logger.error("üö´ Cannot start server - dependencies not available")
            return

        try:
            logger.info(f"üåê Starting WebSocket server on {host}:{port}")

            self.websocket_server = await websockets.serve(
                self._handle_websocket_connection,
                host,
                port,
                max_size=None,  # Pas de limite pour audio
                ping_interval=None  # D√©sactiver ping pour performance
            )

            logger.info("‚úÖ WebSocket server started successfully")
            logger.info("   Waiting for audio streams from FreeSWITCH...")

            # Garder le serveur actif
            await self.websocket_server.wait_closed()

        except Exception as e:
            logger.error(f"‚ùå Failed to start WebSocket server: {e}")
            raise

    async def _handle_websocket_connection(self, websocket):
        """G√®re une connexion WebSocket depuis FreeSWITCH"""
        call_uuid = None
        try:
            # Extraire call_uuid du path: /stream/{UUID}
            # websockets 15+ utilise websocket.request.path
            path = websocket.request.path if hasattr(websocket, 'request') else websocket.path
            call_uuid = path.split('/')[-1]
            logger.info(f"üìû New audio stream for call: {call_uuid[:8]}")

            # Initialiser stream
            self._initialize_stream(call_uuid)

            # Buffer pour accumuler frames
            audio_buffer = b''

            async for message in websocket:
                if isinstance(message, bytes):
                    # Audio brut (SLIN16, 16kHz, mono, 16-bit)
                    audio_buffer += message

                    # Traiter par frames de 30ms
                    bytes_per_frame = self.frame_size * 2  # 2 bytes par sample

                    while len(audio_buffer) >= bytes_per_frame:
                        frame_bytes = audio_buffer[:bytes_per_frame]
                        audio_buffer = audio_buffer[bytes_per_frame:]

                        # Traitement temps r√©el
                        await self._process_audio_frame(call_uuid, frame_bytes)

        except websockets.exceptions.ConnectionClosed:
            if call_uuid:
                logger.info(f"üìû Audio stream closed for call: {call_uuid[:8]}")
        except Exception as e:
            if call_uuid:
                logger.error(f"‚ùå Error handling audio stream for {call_uuid[:8]}: {e}", exc_info=True)
            else:
                logger.error(f"‚ùå Error handling audio stream: {e}", exc_info=True)
        finally:
            if call_uuid:
                self._cleanup_stream(call_uuid)

    def _initialize_stream(self, call_uuid: str):
        """Initialise un stream pour un appel"""
        # Cr√©er recognizer Vosk
        if self.model:
            recognizer = KaldiRecognizer(self.model, self.sample_rate)
            recognizer.SetWords(True)
            self.recognizers[call_uuid] = recognizer
            logger.info(f"üé§ [{call_uuid[:8]}] NEW Vosk recognizer created")
        else:
            logger.error(f"‚ùå [{call_uuid[:8]}] No Vosk model loaded!")

        self.active_streams[call_uuid] = {
            "start_time": time.time(),
            "frame_count": 0,
            "speech_frames": 0,
            "silence_frames": 0,
            "current_speech_duration": 0.0,
            "current_silence_duration": 0.0,
            "in_speech": False,
            "partial_transcription": "",
            "final_transcription": "",
            "last_speech_time": 0.0,
            "audio_warmup_done": False,  # Flag pour ignorer silence initial (RMS=0)
            "first_audio_time": None,  # Timestamp du premier audio r√©el re√ßu
            # Energy gate adaptatif
            "noise_floor_rms": None,  # Plancher de bruit calibr√©
            "calibration_samples": [],  # RMS samples pendant calibration
            "is_calibrating": False  # Mode calibration actif
        }

        # V√©rifier √©tat des autres structures
        num_callbacks = len(self.callbacks)
        num_recognizers = len(self.recognizers)
        num_streams = len(self.active_streams)

        self.stats["active_streams"] += 1
        logger.info(
            f"‚úÖ [{call_uuid[:8]}] Stream initialized: "
            f"callbacks={num_callbacks}, recognizers={num_recognizers}, streams={num_streams}"
        )

    async def _process_audio_frame(self, call_uuid: str, frame_bytes: bytes):
        """Traite une frame audio en temps r√©el"""
        if call_uuid not in self.active_streams:
            return

        start_time = time.time()
        stream_info = self.active_streams[call_uuid]
        recognizer = self.recognizers.get(call_uuid)

        if not recognizer:
            return

        try:
            # VAD - D√©tection activit√© vocale (sur audio ORIGINAL, non filtr√©)
            # WebRTC VAD a √©t√© entra√Æn√© sur audio complet (toutes fr√©quences)
            is_speech = self.vad.is_speech(frame_bytes, self.sample_rate)

            # Mise √† jour statistiques
            stream_info["frame_count"] += 1
            self.stats["total_frames_processed"] += 1

            frame_duration_s = self.frame_duration_ms / 1000.0

            # === AUDIO WARMUP: Ignorer silence initial (frames avec RMS‚âà0) ===
            # FreeSWITCH peut envoyer des frames vides au d√©but du stream
            # On ne compte le silence qu'apr√®s avoir re√ßu du vrai audio
            import numpy as np
            audio_samples_check = np.frombuffer(frame_bytes, dtype=np.int16)
            frame_rms = np.sqrt(np.mean(audio_samples_check.astype(np.float32) ** 2))

            # === ENERGY GATE ADAPTATIF ===
            # Pendant calibration: collecter les samples RMS
            if stream_info["is_calibrating"]:
                stream_info["calibration_samples"].append(frame_rms)

            # Apr√®s calibration: appliquer le filtre de bruit
            noise_floor = stream_info.get("noise_floor_rms")
            if noise_floor and frame_rms < noise_floor:
                # Frame sous le seuil de bruit ‚Üí forcer silence
                is_speech = False

            # Seuil tr√®s bas - juste pour d√©tecter les frames totalement vides (RMS=0)
            # L'audio t√©l√©phonique peut avoir un RMS de 8-15 m√™me en silence
            MIN_AUDIO_RMS = 5
            MAX_WARMUP_FRAMES = 30  # Max 30 frames (~0.9s) de warmup

            if not stream_info["audio_warmup_done"]:
                # Terminer warmup si : audio r√©el OU VAD d√©tecte parole OU timeout warmup
                if frame_rms > MIN_AUDIO_RMS or is_speech or stream_info["frame_count"] >= MAX_WARMUP_FRAMES:
                    stream_info["audio_warmup_done"] = True
                    stream_info["first_audio_time"] = time.time()
                    warmup_frames = stream_info["frame_count"]
                    reason = "RMS" if frame_rms > MIN_AUDIO_RMS else ("VAD" if is_speech else "TIMEOUT")
                    logger.info(
                        f"üîä [{call_uuid[:8]}] Audio warmup complete after {warmup_frames} frames "
                        f"(RMS={frame_rms:.0f}, is_speech={is_speech}, reason={reason})"
                    )

            if is_speech:
                # Parole d√©tect√©e
                stream_info["speech_frames"] += 1
                stream_info["current_speech_duration"] += frame_duration_s
                stream_info["current_silence_duration"] = 0.0
                stream_info["last_speech_time"] = time.time()
                self.stats["speech_frames"] += 1

                if not stream_info["in_speech"]:
                    # D√©but de parole
                    if stream_info["current_speech_duration"] >= self.speech_start_threshold:
                        stream_info["in_speech"] = True
                        logger.debug(f"üó£Ô∏è Speech START detected: {call_uuid[:8]}")
                        await self._notify_speech_start(call_uuid)

            else:
                # Silence d√©tect√©
                stream_info["silence_frames"] += 1
                stream_info["current_speech_duration"] = max(0, stream_info["current_speech_duration"] - frame_duration_s)
                self.stats["silence_frames"] += 1

                # NE PAS compter le silence pendant le warmup (√©vite faux positifs)
                if stream_info["audio_warmup_done"]:
                    stream_info["current_silence_duration"] += frame_duration_s
                else:
                    # Pendant warmup, reset silence pour √©viter accumulation
                    stream_info["current_silence_duration"] = 0.0

                if stream_info["in_speech"]:
                    # V√©rifier si fin de parole
                    if stream_info["current_silence_duration"] >= self.silence_threshold:
                        stream_info["in_speech"] = False
                        logger.info(f"ü§ê Speech END detected: {call_uuid[:8]} (silence: {stream_info['current_silence_duration']:.1f}s, threshold: {self.silence_threshold}s)")
                        await self._notify_speech_end(call_uuid)
                    else:
                        # Log progression du silence
                        if stream_info["current_silence_duration"] % 0.5 < frame_duration_s:  # Log tous les 0.5s
                            logger.debug(f"‚è±Ô∏è Silence accumulating: {call_uuid[:8]} ({stream_info['current_silence_duration']:.1f}s / {self.silence_threshold}s)")
                else:
                    # NOUVEAU: D√©tecter paroles courtes (< 500ms) qui ne triggent pas in_speech
                    # Si on a re√ßu une transcription FINAL et qu'on a du silence suffisant
                    if stream_info.get("final_transcription") and stream_info["current_silence_duration"] >= self.silence_threshold:
                        logger.info(
                            f"ü§ê Speech END detected (short utterance): {call_uuid[:8]} "
                            f"(transcription: '{stream_info['final_transcription']}')"
                        )
                        await self._notify_speech_end(call_uuid)
                        stream_info["final_transcription"] = None  # Reset pour √©viter double d√©tection

            # Audio brut envoy√© directement √† Vosk (filtres d√©sactiv√©s)
            # Utiliser les valeurs RMS d√©j√† calcul√©es dans le warmup check
            audio_rms = frame_rms
            audio_max = np.max(np.abs(audio_samples_check))

            # Log toutes les 50 frames (~1.5s) pour voir l'√©tat
            if stream_info["frame_count"] % 50 == 0:
                warmup_status = "‚úÖ" if stream_info["audio_warmup_done"] else "‚è≥WARMUP"
                logger.info(
                    f"üîä [{call_uuid[:8]}] Audio stats: frame={stream_info['frame_count']}, "
                    f"RMS={audio_rms:.0f}, MAX={audio_max}, "
                    f"in_speech={stream_info['in_speech']}, "
                    f"silence_dur={stream_info['current_silence_duration']:.1f}s, "
                    f"warmup={warmup_status}"
                )

            # ASR - Transcription streaming avec boost audio
            # Boost pour am√©liorer la qualit√© de transcription Vosk
            AUDIO_BOOST_FACTOR = 3.3  # Multiplier le volume par 3.3

            # Appliquer boost avec clipping pour √©viter overflow int16
            boosted_samples = np.clip(
                audio_samples_check.astype(np.float32) * AUDIO_BOOST_FACTOR,
                -32768, 32767
            ).astype(np.int16)
            boosted_frame = boosted_samples.tobytes()

            if recognizer.AcceptWaveform(boosted_frame):
                # Transcription finale
                result = json.loads(recognizer.Result())
                text = result.get("text", "").strip()

                # IMPORTANT: Envoyer le FINAL m√™me si text est vide!
                # Sinon Phase 3 attend ind√©finiment un FINAL qui ne viendra jamais
                stream_info["final_transcription"] = text if text else None

                if text:
                    self.stats["transcriptions"] += 1

                latency_ms = (time.time() - start_time) * 1000
                if text:
                    self._update_latency_stats(latency_ms)

                # Log avec info sur in_speech state
                in_speech_state = "IN_SPEECH" if stream_info["in_speech"] else "SILENCE"
                if text:
                    logger.info(f"üìù FINAL transcription [{call_uuid[:8]}]: '{text}' ({latency_ms:.1f}ms) [VAD state: {in_speech_state}, silence_duration: {stream_info['current_silence_duration']:.1f}s]")
                else:
                    logger.info(f"üìù FINAL transcription [{call_uuid[:8]}]: (empty - Vosk couldn't transcribe) [VAD state: {in_speech_state}, RMS={audio_rms:.0f}]")

                # Envoyer callback FINAL (m√™me si vide)
                await self._notify_transcription(call_uuid, text, "final", latency_ms)

            else:
                # Transcription partielle
                partial_result = json.loads(recognizer.PartialResult())
                partial_text = partial_result.get("partial", "").strip()

                if partial_text and partial_text != stream_info["partial_transcription"]:
                    stream_info["partial_transcription"] = partial_text

                    latency_ms = (time.time() - start_time) * 1000
                    logger.info(f"üìù PARTIAL [{call_uuid[:8]}]: '{partial_text}' (RMS={audio_rms:.0f}, frame={stream_info['frame_count']})")
                    await self._notify_transcription(call_uuid, partial_text, "partial", latency_ms)

        except Exception as e:
            logger.error(f"‚ùå Error processing frame for {call_uuid[:8]}: {e}")

    def _update_latency_stats(self, latency_ms: float):
        """Met √† jour stats de latence"""
        if self.stats["transcriptions"] == 1:
            self.stats["avg_latency_ms"] = latency_ms
        else:
            # Moyenne mobile
            self.stats["avg_latency_ms"] = self.stats["avg_latency_ms"] * 0.9 + latency_ms * 0.1

    async def _notify_speech_start(self, call_uuid: str):
        """Notifie d√©but de parole (pour barge-in)"""
        if call_uuid in self.callbacks:
            try:
                callback = self.callbacks[call_uuid]
                event_data = {
                    "event": "speech_start",
                    "call_uuid": call_uuid,
                    "timestamp": time.time()
                }

                if asyncio.iscoroutinefunction(callback):
                    await callback(event_data)
                else:
                    callback(event_data)

            except Exception as e:
                logger.error(f"‚ùå Callback error (speech_start): {e}")

    async def _notify_speech_end(self, call_uuid: str):
        """Notifie fin de parole"""
        if call_uuid in self.callbacks:
            try:
                callback = self.callbacks[call_uuid]
                stream_info = self.active_streams[call_uuid]

                event_data = {
                    "event": "speech_end",
                    "call_uuid": call_uuid,
                    "timestamp": time.time(),
                    "silence_duration": stream_info["current_silence_duration"]
                }

                if asyncio.iscoroutinefunction(callback):
                    await callback(event_data)
                else:
                    callback(event_data)

            except Exception as e:
                logger.error(f"‚ùå Callback error (speech_end): {e}")

    def start_noise_calibration(self, call_uuid: str):
        """
        D√©marre la calibration du bruit de fond.
        Appel√© au d√©but de Phase 2 hello pendant que le robot parle.
        """
        if call_uuid in self.active_streams:
            self.active_streams[call_uuid]["is_calibrating"] = True
            self.active_streams[call_uuid]["calibration_samples"] = []
            logger.info(f"üéöÔ∏è [{call_uuid[:8]}] Noise calibration STARTED")

    def stop_noise_calibration(self, call_uuid: str) -> float:
        """
        Arr√™te la calibration et calcule le plancher de bruit.
        Accepte N'IMPORTE QUEL nombre de samples (m√™me 1 sample c'est mieux que rien).

        Logique intelligente:
        - 0 samples ‚Üí fallback 500 (par s√©curit√©)
        - 1-10 samples ‚Üí moyenne des samples
        - 10+ samples ‚Üí percentile 90 (optimal)

        Returns:
            float: Le noise floor calcul√© (threshold), ou 0 si pas de samples
        """
        noise_floor_threshold = 0.0
        MIN_NOISE_THRESHOLD = 500  # Fallback absolu si pas assez de donn√©es

        if call_uuid in self.active_streams:
            stream_info = self.active_streams[call_uuid]
            stream_info["is_calibrating"] = False

            samples = stream_info["calibration_samples"]
            num_samples = len(samples)

            logger.info(f"üéöÔ∏è [{call_uuid[:8]}] stop_noise_calibration called: {num_samples} samples collected")

            if num_samples == 0:
                # Aucun sample (barge-in instantan√©?) ‚Üí fallback
                noise_floor_threshold = MIN_NOISE_THRESHOLD
                stream_info["noise_floor_rms"] = noise_floor_threshold
                logger.warning(
                    f"‚ö†Ô∏è [{call_uuid[:8]}] Noise calibration: 0 samples ‚Üí "
                    f"using FALLBACK threshold={noise_floor_threshold:.0f}"
                )

            elif num_samples < 10:
                # Peu de samples (1-9) ‚Üí utiliser moyenne simple
                import numpy as np
                avg_rms = np.mean(samples)
                noise_floor_threshold = max(avg_rms * 4, MIN_NOISE_THRESHOLD)
                stream_info["noise_floor_rms"] = noise_floor_threshold
                logger.info(
                    f"üéöÔ∏è [{call_uuid[:8]}] Noise calibration PARTIAL: "
                    f"samples={num_samples} (LOW, using AVG), avg={avg_rms:.0f}, "
                    f"threshold={noise_floor_threshold:.0f} (avg x4, min={MIN_NOISE_THRESHOLD})"
                )

            else:
                # Suffisamment de samples (10+) ‚Üí utiliser percentile 90 (optimal)
                sorted_samples = sorted(samples)
                percentile_90_idx = int(len(sorted_samples) * 0.9)
                noise_floor = sorted_samples[percentile_90_idx]
                noise_floor_threshold = max(noise_floor * 4, MIN_NOISE_THRESHOLD)
                stream_info["noise_floor_rms"] = noise_floor_threshold
                logger.info(
                    f"üéöÔ∏è [{call_uuid[:8]}] Noise calibration COMPLETE: "
                    f"samples={num_samples} (GOOD), p90={noise_floor:.0f}, "
                    f"threshold={noise_floor_threshold:.0f} (p90 x4, min={MIN_NOISE_THRESHOLD})"
                )

        else:
            logger.warning(f"‚ö†Ô∏è [{call_uuid[:8]}] stop_noise_calibration: call_uuid not in active_streams")

        logger.info(f"üéöÔ∏è [{call_uuid[:8]}] stop_noise_calibration returning: {noise_floor_threshold:.0f}")
        return noise_floor_threshold

    def set_noise_floor(self, call_uuid: str, noise_floor_rms: float):
        """
        Applique un noise floor calibr√© √† un stream existant.
        Utilis√© pour persister le threshold entre les phases.

        Args:
            call_uuid: UUID de l'appel
            noise_floor_rms: Le threshold RMS √† appliquer
        """
        if call_uuid in self.active_streams and noise_floor_rms > 0:
            self.active_streams[call_uuid]["noise_floor_rms"] = noise_floor_rms
            logger.info(f"üéöÔ∏è [{call_uuid[:8]}] Noise floor SET: threshold={noise_floor_rms:.0f}")

    async def _notify_transcription(self, call_uuid: str, text: str, transcription_type: str, latency_ms: float):
        """Notifie transcription"""
        logger.debug(f"üîî [{call_uuid[:8]}] _notify_transcription called: type={transcription_type}, text='{text[:50]}'")
        logger.debug(f"üîî [{call_uuid[:8]}] Registered callbacks: {list(self.callbacks.keys())}")

        if call_uuid in self.callbacks:
            try:
                callback = self.callbacks[call_uuid]
                logger.debug(f"üîî [{call_uuid[:8]}] Calling transcription callback (type={transcription_type})")

                event_data = {
                    "event": "transcription",
                    "call_uuid": call_uuid,
                    "text": text,
                    "type": transcription_type,  # "final" ou "partial"
                    "latency_ms": latency_ms,
                    "timestamp": time.time()
                }

                if asyncio.iscoroutinefunction(callback):
                    await callback(event_data)
                else:
                    callback(event_data)

            except Exception as e:
                logger.error(f"‚ùå Callback error (transcription): {e}", exc_info=True)
        else:
            logger.warning(f"‚ö†Ô∏è [{call_uuid[:8]}] No callback registered for transcription (UUID: {call_uuid})")

    def register_callback(self, call_uuid: str, callback: Callable):
        """
        Enregistre un callback pour un appel

        Args:
            call_uuid: UUID de l'appel
            callback: Fonction √† appeler (peut √™tre async)
        """
        logger.debug(f"üîß Registering callback for UUID: {call_uuid} (short: {call_uuid[:8]})")
        logger.debug(f"üîß Callback function: {callback.__name__ if hasattr(callback, '__name__') else callback}")
        logger.debug(f"üîß Current callbacks before: {list(self.callbacks.keys())}")

        self.callbacks[call_uuid] = callback

        logger.debug(f"‚úÖ Callback registered for {call_uuid[:8]}")
        logger.debug(f"üîß Current callbacks after: {list(self.callbacks.keys())}")

    def unregister_callback(self, call_uuid: str):
        """D√©senregistre callback"""
        logger.debug(f"üîß Unregistering callback for UUID: {call_uuid} (short: {call_uuid[:8]})")
        logger.debug(f"üîß Current callbacks before: {list(self.callbacks.keys())}")

        if call_uuid in self.callbacks:
            del self.callbacks[call_uuid]
            logger.debug(f"‚ùå Callback unregistered for {call_uuid[:8]}")
        else:
            logger.warning(f"‚ö†Ô∏è No callback to unregister for {call_uuid[:8]}")

        logger.debug(f"üîß Current callbacks after: {list(self.callbacks.keys())}")

    def reset_recognizer(self, call_uuid: str):
        """
        R√©initialise le recognizer Vosk pour vider le buffer audio

        Utilis√© apr√®s un barge-in pour √©viter l'accumulation de transcriptions partielles.
        Bas√© sur la m√©thode Reset() de KaldiRecognizer (vosk-api).

        Args:
            call_uuid: UUID de l'appel
        """
        if call_uuid in self.recognizers:
            try:
                self.recognizers[call_uuid].Reset()
                logger.debug(f"[{call_uuid[:8]}] üîÑ Vosk recognizer reset (buffer cleared)")

                # R√©initialiser aussi les transcriptions partielles dans stream_info
                if call_uuid in self.active_streams:
                    self.active_streams[call_uuid]["partial_transcription"] = ""
                    self.active_streams[call_uuid]["final_transcription"] = ""

            except Exception as e:
                logger.error(f"[{call_uuid[:8]}] ‚ùå Failed to reset recognizer: {e}")
        else:
            logger.warning(f"[{call_uuid[:8]}] ‚ö†Ô∏è Cannot reset - recognizer not found")

    def _cleanup_stream(self, call_uuid: str):
        """Nettoie un stream"""
        # Log √©tat avant cleanup
        had_stream = call_uuid in self.active_streams
        had_recognizer = call_uuid in self.recognizers
        frame_count = 0
        if had_stream:
            frame_count = self.active_streams[call_uuid].get("frame_count", 0)

        logger.info(
            f"üßπ [{call_uuid[:8]}] Cleanup stream: "
            f"had_stream={had_stream}, had_recognizer={had_recognizer}, frames={frame_count}"
        )

        if call_uuid in self.active_streams:
            del self.active_streams[call_uuid]

        if call_uuid in self.recognizers:
            # IMPORTANT: Vider le buffer interne de Vosk avant de supprimer
            # Sinon l'√©tat peut s'accumuler et causer des probl√®mes
            try:
                recognizer = self.recognizers[call_uuid]
                # FinalResult() vide le buffer et retourne la derni√®re transcription
                final = recognizer.FinalResult()
                logger.debug(f"üßπ [{call_uuid[:8]}] Vosk buffer flushed: {final[:50] if final else 'empty'}...")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è [{call_uuid[:8]}] Error flushing Vosk buffer: {e}")
            del self.recognizers[call_uuid]

        # ‚ùå NE PAS supprimer le callback automatiquement !
        # Le callback est g√©r√© explicitement par register/unregister
        # Sinon, quand une connexion WebSocket se ferme (ex: AMD),
        # elle supprime le callback de la phase suivante (Phase 2/3)
        #
        # if call_uuid in self.callbacks:
        #     del self.callbacks[call_uuid]

        self.stats["active_streams"] = len(self.active_streams)
        logger.debug(f"üßπ [{call_uuid[:8]}] Stream cleanup completed (callback preserved)")

    def get_stats(self) -> Dict[str, Any]:
        """Retourne statistiques"""
        return {
            **self.stats,
            "is_available": self.is_available,
            "active_streams_list": list(self.active_streams.keys())
        }


# Instance globale
streaming_asr = StreamingASR()


# Fonction helper pour d√©marrer le serveur
async def start_streaming_asr_server():
    """D√©marre le serveur ASR streaming"""
    if streaming_asr.is_available:
        await streaming_asr.start_server()
    else:
        logger.error("‚ùå Cannot start streaming ASR server - dependencies not available")
